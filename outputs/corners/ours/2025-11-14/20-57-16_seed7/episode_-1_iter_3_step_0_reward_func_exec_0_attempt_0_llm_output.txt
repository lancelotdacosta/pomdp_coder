Certainly! The goal is to improve the `reward_func` to better model the reward conditioned on the state, action, and next state transitions. Based on the provided samples, it seems that the main issue is with the reward structure when the agent moves onto a goal cell. Specifically, the agent should receive a smaller reward for initially moving onto the goal and a larger reward when the episode is done.

Here's the improved `reward_func`:

```python
def reward_func(state: MinigridState, action: int, next_state: MinigridState) -> Tuple[float, bool]:
    """
    Args:
        state (MinigridState): the state of the environment
        action (int): the action to be executed
        next_state (MinigridState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """

    # Define a helper function to check if the agent has reached a goal
    def is_agent_on_goal(state: MinigridState) -> bool:
        agent_pos = state.agent_pos
        grid = state.grid
        return grid[agent_pos[0], agent_pos[1]] == ObjectTypes.goal

    # Check if the episode is done
    done = is_agent_on_goal(next_state)

    # Initialize reward
    reward = 0.0

    # Check if the agent has picked up or dropped an object
    if state.carrying != next_state.carrying:
        if next_state.carrying is not None:
            reward += 1.0  # Reward for picking up an object
        else:
            reward += 1.0  # Reward for dropping an object
    
    # Check if the agent has moved onto a goal cell
    if not is_agent_on_goal(state) and is_agent_on_goal(next_state):
        reward += 1.0  # Smaller reward for reaching the goal cell initially
    
    # If the agent reaches the goal, give a large positive reward
    if done:
        reward += 10.0

    # Ensure that the agent does not get the large reward multiple times
    if is_agent_on_goal(state) and is_agent_on_goal(next_state):
        reward = 0.0  # Reset reward to avoid double counting

    return reward, done
```

### Explanation:

1. **Helper Function `is_agent_on_goal`**:
   - This function checks if the agent is on a goal cell by comparing the agent's position with the grid values.

2. **Episode Done Check**:
   - The episode is considered done if the agent is on a goal cell in the next state.

3. **Reward Initialization**:
   - The reward is initialized to 0.0.

4. **Object Interaction Rewards**:
   - If the agent picks up or drops an object (i.e., changes in `carrying`), a reward of 1.0 is given.

5. **Initial Goal Cell Reward**:
   - If the agent moves onto a goal cell for the first time, a smaller reward of 1.0 is given.

6. **Large Reward for Reaching Goal**:
   - If the episode is done (agent is on a goal cell), a large positive reward of 10.0 is given.

7. **Avoid Double Counting**:
   - If the agent is already on a goal cell and remains there, the reward is reset to 0.0 to avoid double counting the large reward.

This should better align with the observed distribution and ensure that the reward structure is more consistent with the real-world samples provided.