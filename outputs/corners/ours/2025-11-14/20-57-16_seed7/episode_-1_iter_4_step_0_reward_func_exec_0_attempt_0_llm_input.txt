#define system
You are a robot exploring its environment. 



Your goal is to model the reward conditioned on state, action, next state transition of the world in python. 

You have tried it before and came up with one partially correct solution, but it is not perfect. 

The observed distribution disagrees with the generated model in several cases.
You need to improve your code to come closer to the true distribution.

Environment Description: 
Goal Description: 

Here is a solution you came up with before. 

```
# type: ignore
from __future__ import annotations

from dataclasses import dataclass
from enum import IntEnum
from typing import Any, List, Optional, Tuple

import numpy as np
from numpy.typing import NDArray

AGENT_DIR_TO_STR = {0: ">", 1: "V", 2: "<", 3: "^"}
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

SEE_THROUGH_WALLS = True


class ObjectTypes(IntEnum):
    unseen = 0
    empty = 1
    wall = 2
    open_door = 4
    closed_door = 5
    locked_door = 6
    key = 7
    ball = 8
    box = 9
    goal = 10
    lava = 11
    agent = 12


class Direction(IntEnum):
    facing_right = 0
    facing_down = 1
    facing_left = 2
    facing_up = 3


class Actions(IntEnum):
    left = 0  # Turn left
    right = 1  # Turn right
    forward = 2  # Move forward
    pickup = 3  # Pick up an object
    drop = 4  # Drop an object
    toggle = 5  # Toggle/activate an object
    done = 6  # Done completing the task


@dataclass
class MinigridObservation(Observation):
    """
    Args:
        `image`: field of view in front of the agent.

        `agent_pos`: agent's position in the real world. It differs from the position
                     in the observation grid.
        `agent_dir`: agent's direction in the real world. It differs from the direction
                     of the agent in the observation grid.
        `carrying`: what the agent is carrying at the moment.
    """

    image: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int] = None


@dataclass
class MinigridState(State):
    """An agent exists in an indoor multi-room environment represented by a
    grid."""

    grid: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int]

    @property
    def front_pos(self) -> Tuple[int, int]:
        """Get the position of the cell that is right in front of the agent."""

        return (
            np.array(self.agent_pos) + np.array(DIR_TO_VEC[self.agent_dir])
        ).tolist()

    @property
    def width(self) -> int:
        return self.grid.shape[0]

    @property
    def height(self) -> int:
        return self.grid.shape[1]

    def get_type_indices(self, type: int) -> List[Tuple[int, int]]:
        idxs = np.where(self.grid == type)  # Returns (row_indices, col_indices)
        return list(zip(idxs[0], idxs[1]))  # Combine row and column indices


    def get_field_of_view(self, view_size: int) -> NDArray[np.int8]:
        """Returns the field of view in front of the agent.

        DO NOT modify this function.
        """

        # Get the extents of the square set of tiles visible to the agent
        # Facing right
        if self.agent_dir == 0:
            topX = self.agent_pos[0]
            topY = self.agent_pos[1] - view_size // 2
        # Facing down
        elif self.agent_dir == 1:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1]
        # Facing left
        elif self.agent_dir == 2:
            topX = self.agent_pos[0] - view_size + 1
            topY = self.agent_pos[1] - view_size // 2
        # Facing up
        elif self.agent_dir == 3:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1] - view_size + 1
        else:
            assert False, "invalid agent direction"

        fov = np.full((view_size, view_size), ObjectTypes.wall, dtype=self.grid.dtype)

        # Compute the overlapping region in the grid.
        gx0 = max(topX, 0)
        gy0 = max(topY, 0)
        gx1 = min(topX + view_size, self.grid.shape[0])
        gy1 = min(topY + view_size, self.grid.shape[1])

        # Determine where the overlapping region goes in the padded array.
        px0 = max(0, -topX)
        py0 = max(0, -topY)

        # Copy the overlapping slice.
        fov[px0 : px0 + (gx1 - gx0), py0 : py0 + (gy1 - gy0)] = self.grid[
            gx0:gx1, gy0:gy1
        ]

        for _ in range(self.agent_dir + 1):
            # Rotate left
            fov = np.rot90(fov.T, k=1).T

        agent_pos = (self.grid.shape[0] // 2, self.grid.shape[1] - 1)
        self.grid[agent_pos] = ObjectTypes.agent

        return fov



def reward_func(state, action, next_state):
    """
    Args:
        state (MinigridState): the state of the environment
        action (int): the action to be executed
        next_state (MinigridState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define a helper function to check if the agent has reached a goal
    def is_agent_on_goal(state):
        agent_pos = state.agent_pos
        grid = state.grid
        return grid[agent_pos[0], agent_pos[1]] == ObjectTypes.goal

    # Check if the episode is done
    done = is_agent_on_goal(next_state)

    # Initialize reward
    reward = 0.0

    # Check if the agent has picked up or dropped an object
    if state.carrying != next_state.carrying:
        if next_state.carrying is not None:
            reward += 1.0  # Reward for picking up an object
        else:
            reward += 1.0  # Reward for dropping an object
    
    # Check if the agent has moved onto a goal cell
    if not is_agent_on_goal(state) and is_agent_on_goal(next_state):
        reward += 1.0  # Smaller reward for reaching the goal cell initially
    
    # If the agent reaches the goal, give a small positive reward
    if done:
        reward += 1.0

    return reward, done

```


Here are some samples from the real world that were impossible under your model
Input MinigridState: agent_pos=(np.int64(7), np.int64(1))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(1))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output int: 1
Output bool: True



And here are some samples from your code under the same conditions
Input MinigridState: agent_pos=(np.int64(7), np.int64(1))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(1))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output float: 2.0
Output bool: True



Here are some samples from the real world that were impossible under your model
Input MinigridState: agent_pos=(np.int64(8), np.int64(2))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(1))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output int: 1
Output bool: True



And here are some samples from your code under the same conditions
Input MinigridState: agent_pos=(np.int64(8), np.int64(2))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(1))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output float: 2.0
Output bool: True



Here are some samples from the real world that were impossible under your model
Input MinigridState: agent_pos=(np.int64(7), np.int64(8))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(8))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output int: 1
Output bool: True



And here are some samples from your code under the same conditions
Input MinigridState: agent_pos=(np.int64(7), np.int64(8))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(8), np.int64(8))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output float: 2.0
Output bool: True



Here are some samples from the real world that were impossible under your model
Input MinigridState: agent_pos=(np.int64(1), np.int64(2))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(1), np.int64(1))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output int: 1
Output bool: True



And here are some samples from your code under the same conditions
Input MinigridState: agent_pos=(np.int64(1), np.int64(2))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Input MinigridState: agent_pos=(np.int64(1), np.int64(1))
agent_dir=3
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2, 10,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Output float: 2.0
Output bool: True





Explain what you believe is the reward conditioned on state, action, next state transition in english, then improve your code to better model the true distribution.

Please implement the code for the following the template. 
You must implement the ‘ reward_func ‘ function. 

The code needs to be directly runnable with (state, action, next_state) and return (reward, done). 

Do not output examples. 
Do not create any new classes. 
Do not rewrite existing classes.
Do not import any new modules from anywhere.
Do not list out specific indices that overfit to the examples, but include ranges.
Put the ‘ reward_func ‘ function in a python code block.
