The observed discrepancies suggest that the current reward function is overly simplistic and does not accurately capture the nuances of the environment. Specifically, the reward for reaching a goal cell and the reward for picking up or dropping objects are being double-counted in some cases, leading to an inflated reward.

To improve the reward function, we need to ensure that the rewards for reaching the goal and for picking up/dropping objects are correctly attributed only once. Additionally, we should verify that the agent's movement and actions are consistent with the expected behavior in the environment.

Here's the improved `reward_func`:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (MinigridState): the state of the environment
        action (int): the action to be executed
        next_state (MinigridState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define a helper function to check if the agent has reached a goal
    def is_agent_on_goal(state):
        agent_pos = state.agent_pos
        grid = state.grid
        return grid[agent_pos[0], agent_pos[1]] == ObjectTypes.goal

    # Check if the episode is done
    done = is_agent_on_goal(next_state)

    # Initialize reward
    reward = 0.0

    # Check if the agent has picked up or dropped an object
    if state.carrying != next_state.carrying:
        if next_state.carrying is not None:
            reward += 1.0  # Reward for picking up an object
        else:
            reward += 1.0  # Reward for dropping an object
    
    # Check if the agent has moved onto a goal cell
    if not is_agent_on_goal(state) and is_agent_on_goal(next_state):
        reward += 1.0  # Smaller reward for reaching the goal cell initially
    
    # If the agent reaches the goal, give a small positive reward
    if done:
        reward += 1.0

    # Ensure the total reward does not exceed 1.0 for reaching the goal
    if done and reward > 1.0:
        reward = 1.0

    return reward, done
```

### Explanation:

1. **Agent on Goal Check**: The function `is_agent_on_goal` checks if the agent is on a goal cell.
2. **Episode Done Check**: The `done` flag is set to `True` if the agent is on a goal cell in the next state.
3. **Initialize Reward**: The initial reward is set to 0.0.
4. **Pick Up/Drop Object Check**: If the agent picks up or drops an object, a reward of 1.0 is given.
5. **Move Onto Goal Cell**: If the agent moves onto a goal cell, a reward of 1.0 is given.
6. **Goal Reach Reward**: If the agent reaches the goal (i.e., `done` is `True`), an additional reward of 1.0 is given.
7. **Ensure Total Reward**: If the total reward exceeds 1.0 for reaching the goal, it is capped at 1.0 to avoid double-counting.

This should better align the reward function with the true distribution observed in the environment.