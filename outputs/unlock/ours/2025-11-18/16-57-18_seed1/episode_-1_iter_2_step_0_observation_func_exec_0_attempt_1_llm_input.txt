#define system
You are a robot exploring its environment. 

Environment Description: Unlock door with key to reach the goal square
Goal Description: 

Your goal is to model the the distribution of observations conditioned on a state. 
You need to implement the python code to model the world, as seen in the provided experiences. 
Please follow the template to implement the code. 
The code needs to be directly runnable (state) and return a sample (observation). 


Below are a few samples from the environment distribution. These are only samples from a larger distribution that your should model.

Input MinigridState: agent_pos=(np.int64(2), np.int64(3))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  7,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  6,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1, 10,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2, ],
]

Input Actions: 1
Output MinigridObservation: agent_pos=(np.int64(2), np.int64(3))
agent_dir=1
carrying=None
grid=[
[ 2,  1,  1, ],
[ 2,  7, 12, ],
[ 2,  1,  1, ],
]



Input MinigridState: agent_pos=(np.int64(4), np.int64(3))
agent_dir=1
carrying=7
grid=[
[ 2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  6,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1, 10,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridObservation: agent_pos=(np.int64(4), np.int64(3))
agent_dir=1
carrying=7
grid=[
[ 2,  2,  6, ],
[ 2,  1, 12, ],
[ 2,  1,  1, ],
]



Input MinigridState: agent_pos=(np.int64(7), np.int64(3))
agent_dir=0
carrying=7
grid=[
[ 2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  4,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1, 10,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridObservation: agent_pos=(np.int64(7), np.int64(3))
agent_dir=0
carrying=7
grid=[
[ 1,  1,  1, ],
[ 1, 10, 12, ],
[ 1,  1,  1, ],
]



Input MinigridState: agent_pos=(np.int64(2), np.int64(3))
agent_dir=1
carrying=7
grid=[
[ 2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  6,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1, 10,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2, ],
]

Input Actions: 3
Output MinigridObservation: agent_pos=(np.int64(2), np.int64(3))
agent_dir=1
carrying=7
grid=[
[ 2,  1,  1, ],
[ 2,  1, 12, ],
[ 2,  1,  1, ],
]



Input MinigridState: agent_pos=(np.int64(2), np.int64(3))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  7,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  6,  2,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  1,  1, 10,  1,  2, ],
[ 2,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridObservation: agent_pos=(np.int64(2), np.int64(3))
agent_dir=0
carrying=None
grid=[
[ 1,  1,  1, ],
[ 1,  1, 12, ],
[ 1,  1,  7, ],
]





Here is the template for the observation_func function. Please implement
the reward function following the template. The code needs to be directly
runnable.

‘‘‘
# type: ignore
from __future__ import annotations

from dataclasses import dataclass
from enum import IntEnum
from typing import Any, List, Optional, Tuple

import numpy as np
from numpy.typing import NDArray

AGENT_DIR_TO_STR = {0: ">", 1: "V", 2: "<", 3: "^"}
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

SEE_THROUGH_WALLS = True


class ObjectTypes(IntEnum):
    unseen = 0
    empty = 1
    wall = 2
    open_door = 4
    closed_door = 5
    locked_door = 6
    key = 7
    ball = 8
    box = 9
    goal = 10
    lava = 11
    agent = 12


class Direction(IntEnum):
    facing_right = 0
    facing_down = 1
    facing_left = 2
    facing_up = 3


class Actions(IntEnum):
    left = 0  # Turn left
    right = 1  # Turn right
    forward = 2  # Move forward
    pickup = 3  # Pick up an object
    drop = 4  # Drop an object
    toggle = 5  # Toggle/activate an object
    done = 6  # Done completing the task


@dataclass
class MinigridObservation(Observation):
    """
    Args:
        `image`: field of view in front of the agent.

        `agent_pos`: agent's position in the real world. It differs from the position
                     in the observation grid.
        `agent_dir`: agent's direction in the real world. It differs from the direction
                     of the agent in the observation grid.
        `carrying`: what the agent is carrying at the moment.
    """

    image: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int] = None


@dataclass
class MinigridState(State):
    """An agent exists in an indoor multi-room environment represented by a
    grid."""

    grid: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int]

    @property
    def front_pos(self) -> Tuple[int, int]:
        """Get the position of the cell that is right in front of the agent."""

        return (
            np.array(self.agent_pos) + np.array(DIR_TO_VEC[self.agent_dir])
        ).tolist()

    @property
    def width(self) -> int:
        return self.grid.shape[0]

    @property
    def height(self) -> int:
        return self.grid.shape[1]

    def get_type_indices(self, type: int) -> List[Tuple[int, int]]:
        idxs = np.where(self.grid == type)  # Returns (row_indices, col_indices)
        return list(zip(idxs[0], idxs[1]))  # Combine row and column indices


    def get_field_of_view(self, view_size: int) -> NDArray[np.int8]:
        """Returns the field of view in front of the agent.

        DO NOT modify this function.
        """

        # Get the extents of the square set of tiles visible to the agent
        # Facing right
        if self.agent_dir == 0:
            topX = self.agent_pos[0]
            topY = self.agent_pos[1] - view_size // 2
        # Facing down
        elif self.agent_dir == 1:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1]
        # Facing left
        elif self.agent_dir == 2:
            topX = self.agent_pos[0] - view_size + 1
            topY = self.agent_pos[1] - view_size // 2
        # Facing up
        elif self.agent_dir == 3:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1] - view_size + 1
        else:
            assert False, "invalid agent direction"

        fov = np.full((view_size, view_size), ObjectTypes.wall, dtype=self.grid.dtype)

        # Compute the overlapping region in the grid.
        gx0 = max(topX, 0)
        gy0 = max(topY, 0)
        gx1 = min(topX + view_size, self.grid.shape[0])
        gy1 = min(topY + view_size, self.grid.shape[1])

        # Determine where the overlapping region goes in the padded array.
        px0 = max(0, -topX)
        py0 = max(0, -topY)

        # Copy the overlapping slice.
        fov[px0 : px0 + (gx1 - gx0), py0 : py0 + (gy1 - gy0)] = self.grid[
            gx0:gx1, gy0:gy1
        ]

        for _ in range(self.agent_dir + 1):
            # Rotate left
            fov = np.rot90(fov.T, k=1).T

        agent_pos = (self.grid.shape[0] // 2, self.grid.shape[1] - 1)
        self.grid[agent_pos] = ObjectTypes.agent

        return fov

def observation_func(state, action, empty_obs):
    """
    Args:
        state (MinigridState): the state of the environment
        action (int): the previous action that was taken
        empty_obs (MinigridObservation): an empty observation that needs to be filled and returned
    Returns:
        obs (MinigridObservation): observation of the agent
    """
    raise NotImplementedError
‘‘‘

Explain what you believe is the the distribution of observations conditioned on a state in english.
Additionally, please implement code to model the logic of the world. Please implement the 
code following the template. Only output the definition for ‘ observation_func ‘. 
You must implement the ‘ observation_func ‘ function.
Create any helper function inside the scope of ‘ observation_func ‘. 
Do not create any helper function outside the scope of ‘ observation_func ‘.
Do not output examples usage. 
Do not create any new classes.
Do not rewrite existing classes. 
Do not import any new modules from anywhere.
Do not overfit to the specific samples.
Put the ‘ observation_func ‘ function in a python code block.
#define assistant
The distribution of observations conditioned on a state in this environment is determined by the agent's position, direction, and the objects in its field of view. The observation is a partial view of the grid centered around the agent and rotated based on the agent's direction. The observation grid includes the agent's current position and the objects it can see within its view.

Here is the implementation of the `observation_func` function:

```python
# type: ignore
from __future__ import annotations

import numpy as np
from typing import Tuple, Optional, NDArray

def observation_func(state: MinigridState, action: int, empty_obs: MinigridObservation) -> MinigridObservation:
    """
    Args:
        state (MinigridState): the state of the environment
        action (int): the previous action that was taken
        empty_obs (MinigridObservation): an empty observation that needs to be filled and returned
    Returns:
        obs (MinigridObservation): observation of the agent
    """
    
    # Get the field of view in front of the agent
    view_size = 3  # The size of the observation grid (3x3)
    fov = state.get_field_of_view(view_size)

    # Update the agent's position and direction in the observation
    empty_obs.agent_pos = state.agent_pos
    empty_obs.agent_dir = state.agent_dir
    empty_obs.carrying = state.carrying

    # Set the image part of the observation
    empty_obs.image = fov

    return empty_obs
```

This function calculates the field of view based on the agent's position, direction, and the grid. It then updates the provided `empty_obs` with the agent's position, direction, carrying item, and the field of view. The `view_size` is set to 3, which matches the provided samples.
#define user
Traceback (most recent call last):
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/base_policy.py", line 201, in requery
    exec(code_obj, globals(), local_scope)
  File "generated_code_observation_func200.py", line 7, in <module>
    from typing import Tuple, Optional, NDArray
ImportError: cannot import name 'NDArray' from 'typing' (/usr/lib64/python3.11/typing.py)

