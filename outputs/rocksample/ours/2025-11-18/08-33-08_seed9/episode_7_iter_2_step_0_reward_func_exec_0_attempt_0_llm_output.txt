Certainly! Based on the provided samples and the discrepancies between your current model and the real-world data, it appears that the reward and terminal state conditions need to be re-evaluated, especially for the `EXIT` action. Here is the improved `reward_func` function:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    # Check if the agent is at a rock and samples it
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:  # Good rock
                reward = 10.0
            else:  # Bad rock
                reward = -10.0
        else:
            reward = -10.0  # Sampling when not at a rock

    # Check if the agent exits the grid
    if action == RockSampleActions.EXIT:
        done = True
        if state.at_rock() is not None and state.rocks[state.at_rock()]:
            reward = 10.0
        elif (state.x, state.y) in ROCK_POSITIONS:
            reward = -10.0
        else:
            reward = 0.0

    # Check if the agent checks a rock
    if action in CHECK_ACTIONS:
        rock_idx = CHECK_ACTIONS.index(action)
        if (state.x, state.y) == ROCK_POSITIONS[rock_idx]:
            reward = -1.0  # Cost of checking a rock

    return reward, done
```

### Explanation:

1. **Sampling Action (`SAMPLE`):**
   - If the agent is at a rock and samples it:
     - `+10.0` reward for sampling a good rock.
     - `-10.0` reward for sampling a bad rock.
   - `-10.0` penalty if the agent tries to sample when not at a rock.

2. **Exit Action (`EXIT`):**
   - The episode always terminates when the `EXIT` action is taken.
   - If the agent is at a rock and exits:
     - `+10.0` reward if the rock is good.
     - `-10.0` reward if the rock is bad.
   - If the agent exits from any other position (not at a rock):
     - `0.0` reward.

3. **Check Rock Actions (`CHECK_ROCK_0` and `CHECK_ROCK_1`):**
   - If the agent checks a rock:
     - `-1.0` cost for checking a rock, regardless of whether it is good or bad.
   - This action does not terminate the episode.

This function should better align with the observed real-world samples and provide a more accurate reward structure conditioned on the state, action, and next state transitions.