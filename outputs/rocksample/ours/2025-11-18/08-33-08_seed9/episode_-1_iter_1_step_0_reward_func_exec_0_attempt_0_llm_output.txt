```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Check if the agent has exited the grid
    if action == RockSampleActions.EXIT:
        return 0.0, True

    # Check if the agent sampled a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # If the rock's status changed, it means the sample was successful
            if next_state.rocks[rock_idx] != state.rocks[rock_idx]:
                return 10.0, False
            else:
                return -1.0, False
        else:
            return -1.0, False

    # Check if the agent moved to a new position
    if (state.x, state.y) != (next_state.x, next_state.y):
        return 0.0, False

    # Check if the agent performed a check action
    if action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            return 0.0, False
        else:
            return -1.0, False

    # Default reward for other actions
    return 0.0, False
```

### Explanation in English

The reward function should account for the following scenarios:

1. **Exiting the Grid**:
   - If the agent performs the `EXIT` action, the episode is considered done, and the reward is 0.

2. **Sampling a Rock**:
   - If the agent performs the `SAMPLE` action and is at a rock's position, the reward depends on whether the rock's status (good or bad) changes.
   - If the rock's status changes, it indicates a successful sample, and the reward is 10.
   - If the rock's status does not change, it indicates an unsuccessful sample, and the reward is -1.
   - If the agent performs the `SAMPLE` action but is not at a rock's position, the reward is -1.

3. **Moving to a New Position**:
   - If the agent moves to a new position (i.e., the next state is different from the current state), the reward is 0.

4. **Performing a Check Action**:
   - If the agent performs a `CHECK` action and is at a rock's position, the reward is 0.
   - If the agent performs a `CHECK` action but is not at a rock's position, the reward is -1.

5. **Other Actions**:
   - For all other actions, the reward is 0.

This updated function should better model the true distribution of rewards based on the provided examples and environment description.