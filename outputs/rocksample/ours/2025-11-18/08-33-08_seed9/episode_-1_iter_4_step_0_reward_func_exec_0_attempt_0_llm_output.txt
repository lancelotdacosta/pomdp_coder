Based on the provided samples and discrepancies, it seems that the reward function needs to be adjusted to better match the real-world behavior. Here's a detailed explanation of the reward structure conditioned on state, action, and next state transition:

1. **Exit Action**:
   - If the agent performs the `EXIT` action, the episode should end with a reward of 0.

2. **Sample Action**:
   - If the agent performs the `SAMPLE` action at a rock's position, the reward depends on whether the rock's status changes.
   - If the rock's status changes (indicating a successful sample), the reward should be 10.
   - If the rock's status does not change or if the agent is not at a rock's position, the reward should be -10.

3. **Check Actions**:
   - If the agent performs a `CHECK_ROCK` action and is at the rock's position, the reward should be 0.
   - If the agent performs a `CHECK_ROCK` action but is not at the rock's position, the reward should be -1.

4. **Movement Actions**:
   - If the agent moves to a new position, the reward should be 0.
   - If the agent remains in the same position (e.g., trying to move out of bounds), the reward should be 0.

5. **Other Actions**:
   - For any other actions, the reward should be 0.

Here is the improved `reward_func` function:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Check if the agent has exited the grid
    if action == RockSampleActions.EXIT:
        return 0.0, True

    # Check if the agent sampled a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # If the rock's status changed, it means the sample was successful
            if next_state.rocks[rock_idx] != state.rocks[rock_idx]:
                return 10.0, False
            else:
                return -10.0, False
        else:
            return -10.0, False

    # Check if the agent performed a check action
    if action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            return 0.0, False
        else:
            return -1.0, False

    # Check if the agent moved to a new position
    if (state.x, state.y) != (next_state.x, next_state.y):
        return 0.0, False

    # Default reward for other actions
    return 0.0, False
```

This function should now more closely match the true distribution of rewards based on the provided samples and discrepancies.