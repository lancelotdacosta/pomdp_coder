Certainly! Let's analyze the problem and improve the `reward_func` function to better model the true distribution of rewards in the RockSample environment.

### Analysis

1. **Exit Action (Action 5):**
   - The exit action should provide a reward of 10.0 only if the agent is at one of the rock positions. Otherwise, it should provide a reward of 0.0.
   - The episode should always end after the exit action, regardless of the reward.

2. **Check Rock Actions (Actions 6 and 7):**
   - For these actions, the reward should be -1.0 if the agent is not at the corresponding rock position. If the agent is at the correct rock position, the reward should be 0.0.

3. **Sample Action (Action 4):**
   - The sample action should provide a reward of 10.0 if the agent is at a rock position and the rock is good. If the rock is bad or the agent is not at a rock position, the reward should be -10.0.
   - The episode should end after the sample action.

4. **Movement Actions (Actions 0, 1, 2, 3):**
   - Movement actions should always provide a reward of 0.0 and should not end the episode.

### Improved `reward_func` Function

Here is the improved `reward_func` function:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Reward for checking a rock
    if action in CHECK_ACTIONS:
        rock_idx = CHECK_ACTIONS.index(action)
        if state.at_rock() == rock_idx:
            return 0.0, False
        else:
            return -1.0, False

    # Reward for sampling a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None and next_state.rocks[rock_idx]:
            return 10.0, True
        else:
            return -10.0, True

    # Reward for exit action
    if action == RockSampleActions.EXIT:
        if (state.x, state.y) in ROCK_POSITIONS:
            return 10.0, True
        else:
            return 0.0, True

    # No reward for movement actions
    if action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        return 0.0, False

    # Default case (should not happen)
    return 0.0, False
```

### Explanation

1. **Check Rock Actions:**
   - The function now correctly returns a reward of 0.0 if the agent is at the correct rock position and -1.0 if it is not.

2. **Sample Action:**
   - The function now correctly checks if the agent is at a rock position and if the rock is good before providing the reward. It ends the episode after the sample action.

3. **Exit Action:**
   - The function now correctly provides a reward of 10.0 if the agent is at one of the rock positions and 0.0 otherwise. The episode always ends after the exit action.

4. **Movement Actions:**
   - The function correctly provides a reward of 0.0 and does not end the episode for movement actions.

This improved function should better match the true distribution of rewards in the RockSample environment.