Certainly! Let's analyze the discrepancies between the real-world samples and the generated model to improve the `reward_func` function.

### Analysis of Discrepancies

1. **Exit Action at Non-Exit Position**:
   - Real world: If the agent is at a non-exit position (e.g., (4, 2)) and performs the EXIT action, it should not receive any reward.
   - Model: The model incorrectly gives a reward of 10.0 and marks the episode as done.

2. **Movement Out of Bounds**:
   - Real world: If the agent tries to move out of bounds, it should receive a penalty of -1.0.
   - Model: The model correctly handles this case.

3. **Check Rock Actions**:
   - Real world: Performing a check rock action should result in no reward.
   - Model: The model incorrectly gives a penalty of -1.0.

4. **Exit Action at Exit Position**:
   - Real world: If the agent is at the exit position (0, 0) and performs the EXIT action, it should receive no reward but end the episode.
   - Model: The model incorrectly gives a reward of 0.0 and marks the episode as done.

5. **Exit Action at Non-Exit Position**:
   - Real world: If the agent is at a non-exit position (e.g., (4, 4)) and performs the EXIT action, it should receive a reward of 10.0 and end the episode.
   - Model: The model incorrectly gives a reward of 0.0 and does not end the episode.

### Improved `reward_func`

Based on the analysis, we need to adjust the reward function to handle the exit action more correctly and ensure that check rock actions do not penalize the agent. Here is the improved `reward_func`:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    if action == RockSampleActions.SAMPLE:
        # If the agent samples a rock
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:  # Good rock
                reward = 10.0
            else:  # Bad rock
                reward = -10.0
            # Update rock state to bad after sampling
            next_rocks = list(next_state.rocks)
            next_rocks[rock_idx] = False
            next_state.rocks = tuple(next_rocks)
            done = False  # Episode does not end after sampling

    if action == RockSampleActions.EXIT:
        # If the agent exits the environment
        if state.x == 0 and state.y == 0:  # Assuming (0, 0) is the exit position
            done = True
        else:
            reward = 10.0  # Reward for trying to exit from a non-exit position

    if action in CHECK_ACTIONS:
        # If the agent checks a rock, no reward is given
        reward = 0.0

    if action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        # If the agent moves, no reward is given unless it tries to move out of bounds
        if state.x != next_state.x or state.y != next_state.y:
            reward = 0.0
        else:
            reward = -1.0  # Smaller penalty for trying to move out of bounds

    return reward, done
```

### Explanation

1. **Exit Action at Exit Position**:
   - If the agent is at (0, 0) and performs the EXIT action, the episode ends (`done = True`), but no reward is given (`reward = 0.0`).

2. **Exit Action at Non-Exit Position**:
   - If the agent is at a non-exit position and performs the EXIT action, it receives a reward of 10.0 (`reward = 10.0`) and the episode does not end (`done = False`).

3. **Check Rock Actions**:
   - Performing a check rock action results in no reward (`reward = 0.0`).

4. **Movement Out of Bounds**:
   - If the agent tries to move out of bounds, it receives a penalty of -1.0 (`reward = -1.0`).

This should better match the real-world samples and correctly model the reward function.