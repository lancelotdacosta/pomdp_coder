Certainly! Based on the provided samples and the discrepancies between the real world and the model, it seems that the reward and terminal conditions are not correctly captured. Let's analyze the issues and update the `reward_func` accordingly.

### Analysis of Discrepancies

1. **Movement Actions (0, 1, 2, 3):**
   - If the agent attempts to move outside the grid boundaries, the reward is -1.
   - If the agent stays in the same position (no movement), the reward is -1.
   - Otherwise, the reward is 0.

2. **Sample Action (4):**
   - If the agent is at a rock position and samples it:
     - Good rock: +10
     - Bad rock: -10
   - If the agent attempts to sample but is not at a rock position, the reward is -1.

3. **Exit Action (5):**
   - If the agent is at the exit location (0, 0) and exits, the reward is +10 and the episode ends.
   - Otherwise, the reward is 0.

4. **Check Rock Actions (6, 7):**
   - These actions do not provide any immediate reward.

### Updated `reward_func`

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # If the agent exits the grid, the episode is done and the reward is 10
    if action == RockSampleActions.EXIT:
        # Check if the agent is at the exit location (0, 0)
        if state.x == 0 and state.y == 0:
            return 10.0, True
        else:
            return 0.0, False

    # If the agent samples a rock and it's good, the reward is 10, otherwise -10
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                return 10.0, False
            else:
                return -10.0, False
        else:
            # Sampling when not at a rock results in a penalty
            return -1.0, False

    # If the agent checks a rock, there is no immediate reward
    if action in CHECK_ACTIONS:
        return 0.0, False

    # For all other actions (movement), the reward is -1 if the next state is the same as the current state
    if state == next_state:
        return -1.0, False
    else:
        # Check if the next state is within grid bounds
        if 0 <= next_state.x < GRID_SIZE and 0 <= next_state.y < GRID_SIZE:
            return 0.0, False
        else:
            return -1.0, False
```

### Explanation of Changes

1. **Movement Actions:**
   - Ensured that staying in the same position or moving out of bounds results in a reward of -1.
   - Otherwise, the reward is 0.

2. **Sample Action:**
   - Correctly handles sampling at rock positions and penalizes sampling when not at a rock.

3. **Exit Action:**
   - Correctly handles the exit condition and provides the appropriate reward and termination status.

4. **Check Rock Actions:**
   - No immediate reward for these actions, as per the problem description.

This updated `reward_func` should better align with the observed distribution in the real world.