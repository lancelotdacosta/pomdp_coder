The issue in the provided `reward_func` is that it does not correctly handle the penalty for checking a rock when the agent is not at the correct rock position. The real world samples show that the agent should receive a penalty of -1.0 if it attempts to check a rock when it is not at the correct rock position.

Here is the improved `reward_func` function:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Determine if the agent is at a rock
    current_rock_idx = state.at_rock()
    next_rock_idx = next_state.at_rock()

    # Determine the reward based on the action and the transition
    if action == RockSampleActions.SAMPLE:
        if current_rock_idx is not None:
            # If the agent samples a rock, check if the rock was good
            if state.rocks[current_rock_idx]:
                return 10.0, False
            else:
                return -10.0, False
        else:
            # Sampling when not at a rock gives a penalty
            return -1.0, False
    elif action == RockSampleActions.EXIT:
        # Exiting the grid gives a reward of 10.0 if the agent is in the last column (x = 4)
        if next_state.x == 4:
            return 10.0, True
        else:
            return 0.0, False
    elif action in CHECK_ACTIONS:
        # Checking a rock gives no reward if the agent is at the correct rock position
        if current_rock_idx is not None and action == RockSampleActions.CHECK_ROCK_0 + current_rock_idx:
            return 0.0, False
        else:
            # Penalty for checking a rock when not at the correct rock position
            return -1.0, False
    else:
        # Moving actions give no reward
        return 0.0, False
```

### Explanation

1. **Sampling Action (`SAMPLE`)**:
   - If the agent is at a rock, it receives a reward of 10.0 if the rock is good and -10.0 if the rock is bad.
   - If the agent attempts to sample a rock but is not at a rock position, it receives a penalty of -1.0.

2. **Exit Action (`EXIT`)**:
   - The agent receives a reward of 10.0 and the episode ends if it exits the grid from the last column (x = 4).
   - If the agent exits from any other column, it receives no reward and the episode does not end.

3. **Check Rock Actions (`CHECK_ROCK_0`, `CHECK_ROCK_1`)**:
   - If the agent is at the correct rock position and performs the corresponding check action, it receives no reward.
   - If the agent performs a check action but is not at the correct rock position, it receives a penalty of -1.0.

This improved function should better model the true distribution of rewards based on the given examples.