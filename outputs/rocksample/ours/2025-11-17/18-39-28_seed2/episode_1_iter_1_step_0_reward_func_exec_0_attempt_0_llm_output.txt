Certainly! Let's first explain the reward structure based on the provided observations and then improve the `reward_func` function accordingly.

### Explanation of Reward Structure

1. **Exit Action (Action 5)**:
   - If the agent is at one of the rock positions and performs the exit action, it should receive a reward of 10.0 if the rock is bad (`B`). This means the agent gets a positive reward for exiting when it is at a bad rock position.
   - If the agent is not at any rock position and performs the exit action, it should receive a reward of 0.0 and the episode ends.
   - If the agent is at a good rock position and performs the exit action, it should receive a reward of 0.0 and the episode ends.

2. **Sample Action (Action 4)**:
   - If the agent is at a rock position and samples a good rock, it should receive a reward of 10.0.
   - If the agent is at a rock position and samples a bad rock, it should receive a reward of -10.0.
   - If the agent is not at any rock position and samples, it should receive a reward of -1.0.

3. **Check Rock Actions (Actions 6 and 7)**:
   - If the agent is at a rock position and performs the correct check action for that rock, it should receive a reward of 0.0.
   - If the agent is at a rock position and performs the incorrect check action for that rock, it should receive a reward of -1.0.
   - If the agent is not at any rock position and performs a check action, it should receive a reward of -1.0.

4. **Movement Actions**:
   - Movement actions do not provide any reward unless they are invalid (e.g., if the agent tries to move into an obstacle or outside the grid).

5. **Default Case**:
   - For any other action or state transition, the reward should be 0.0.

### Improved `reward_func` Function

Here is the improved `reward_func` function based on the above explanation:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Determine if the agent is at a rock position
    rock_idx = state.at_rock()

    if action == RockSampleActions.SAMPLE:
        if rock_idx is not None:
            if state.rocks[rock_idx]:  # Good rock
                return 10.0, False
            else:  # Bad rock
                return -10.0, False
        else:
            return -1.0, False  # No rock to sample

    if action == RockSampleActions.EXIT:
        if rock_idx is not None:
            if not state.rocks[rock_idx]:  # Bad rock
                return 10.0, True
            else:  # Good rock
                return 0.0, True
        else:
            return 0.0, True  # Not at a rock position

    if action in CHECK_ACTIONS and rock_idx is not None:
        if (action == RockSampleActions.CHECK_ROCK_0 and rock_idx == 0) or (action == RockSampleActions.CHECK_ROCK_1 and rock_idx == 1):
            return 0.0, False
        else:
            return -1.0, False  # Checking a rock that is not present at the position

    if action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        if state.x == next_state.x and state.y == next_state.y:
            return 0.0, False  # No movement
        else:
            return 0.0, False  # Movement reward

    return 0.0, False  # Default case
```

This function should now better model the true distribution based on the provided observations.