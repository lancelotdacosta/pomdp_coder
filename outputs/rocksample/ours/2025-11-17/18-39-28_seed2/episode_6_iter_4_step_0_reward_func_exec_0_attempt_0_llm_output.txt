Based on the provided samples and the environment description, it seems that the `EXIT` action only provides a reward and ends the episode if the agent is at a specific location, likely the exit point of the grid. Additionally, the `SAMPLE` action should only provide a reward if the agent is at a rock position and the rock is good. If the agent is not at a rock position or the rock is bad, it should provide a negative reward. The `CHECK` actions do not change the state or provide any reward, they are just observations.

Let's refine the `reward_func` to better model these conditions:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Check if the agent has exited the grid
    if action == RockSampleActions.EXIT:
        if (state.x, state.y) in [(0, 0), (4, 2)]:  # Example exit points, adjust as needed
            return 10.0, True
        else:
            return 0.0, False

    # Check if the agent is sampling a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:  # Good rock
                return 10.0, False
            else:  # Bad rock
                return -10.0, False
        else:
            return -1.0, False  # Sampling but not at a rock

    # Check if the agent is checking a rock
    if action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            return 0.0, False
        else:
            return -1.0, False  # Checking but not at a rock

    # Default reward for moving actions
    return 0.0, False
```

### Explanation
1. **EXIT Action**:
   - The `EXIT` action should only provide a reward and end the episode if the agent is at one of the specified exit points. In this example, I've used `(0, 0)` and `(4, 2)` as possible exit points. You can adjust these coordinates based on the actual environment description.

2. **SAMPLE Action**:
   - If the agent is at a rock position (`at_rock` returns a valid index), it checks the quality of the rock.
   - If the rock is good, it provides a positive reward (`10.0`).
   - If the rock is bad, it provides a negative reward (`-10.0`).
   - If the agent tries to sample but is not at a rock position, it provides a negative reward (`-1.0`).

3. **CHECK Actions**:
   - These actions do not provide any reward and do not change the state.
   - If the agent is at a rock position, it returns `0.0`.
   - If the agent is not at a rock position, it returns `-1.0`.

4. **Default Reward for Moving Actions**:
   - For all other actions (moving actions), it returns `0.0` and continues the episode.

This function should better align with the observed distribution of rewards and transitions in the real world. Adjust the exit points and any other specific conditions as needed based on your environment.