The issue in the provided `reward_func` function is that the `EXIT` action is always marking the episode as done, regardless of the state or the position of the agent. However, based on the real-world samples, it seems that the episode should only be marked as done if the agent is at a specific exit location when performing the `EXIT` action.

Let's update the `reward_func` function to reflect this behavior:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                # If the agent samples a good rock, it gets a positive reward
                reward = 10.0
            else:
                # If the agent samples a bad rock, it gets a larger negative reward
                reward = -10.0
        else:
            # If the agent samples but there is no rock, it gets a smaller negative reward
            reward = -5.0

    elif action in CHECK_ACTIONS:
        # When checking a rock, the agent receives a small negative reward
        reward = -1.0

    elif action == RockSampleActions.EXIT:
        # If the agent exits from the correct position, the episode is done
        if state.x == 0 and state.y == 0:  # Assuming (0, 0) is the exit location
            done = True

    return reward, done
```

In this updated function:
- The `EXIT` action only sets `done` to `True` if the agent is at the exit location `(0, 0)`.
- The rest of the logic remains the same as in the original function.

This should better align with the real-world samples provided.