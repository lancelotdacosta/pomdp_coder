The observed discrepancies suggest that the reward function is not correctly accounting for certain conditions. Specifically:

1. ** sampling at a rock position should yield a positive reward if the rock is good and a negative reward if the rock is bad.**
2. ** exiting the environment should yield a positive reward if the agent is at a specific exit position and no reward otherwise.**
3. ** checking a rock should yield a small negative reward regardless of the rock's state.**

Let's update the `reward_func` to better match these conditions:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Default reward and done flag
    reward = 0.0
    done = False

    # Handle movement actions
    if action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        # No immediate reward for movement actions
        pass

    # Handle sampling action
    elif action == RockSampleActions.SAMPLE:
        # If the agent is at a rock position, the reward depends on the rock's state
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Positive reward for good rock
            else:
                reward = -10.0  # Negative reward for bad rock

    # Handle exit action
    elif action == RockSampleActions.EXIT:
        # This could be a terminal action with potential reward
        done = True
        # Reward based on the state of the rocks or other conditions
        if (state.x, state.y) == (4, 4):  # Specific exit position
            reward = 10.0

    # Handle check rock actions
    elif action in CHECK_ACTIONS:
        # If the agent is at the rock position being checked, the reward is -1.0
        rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if (state.x, state.y) == ROCK_POSITIONS[rock_idx]:
            reward = -1.0

    return reward, done
```

### Explanation of Changes:

1. **Sampling Action (`RockSampleActions.SAMPLE`)**:
   - The reward now depends on the state of the rock at the current position. If the rock is good (`True`), the reward is `10.0`. If the rock is bad (`False`), the reward is `-10.0`.

2. **Exit Action (`RockSampleActions.EXIT`)**:
   - The episode is marked as done after the exit action.
   - A positive reward of `10.0` is given if the agent is at the specific exit position `(4, 4)`. Otherwise, no reward is given.

3. **Check Rock Actions (`CHECK_ACTIONS`)**:
   - A small negative reward of `-1.0` is given if the agent is at the rock position being checked, regardless of the rock's state.

These changes should better align the model with the observed behavior in the real world.