#define system
You are a robot exploring its environment. 



Your goal is to model the reward conditioned on state, action, next state transition of the world in python. 

You have tried it before and came up with one partially correct solution, but it is not perfect. 

The observed distribution disagrees with the generated model in several cases.
You need to improve your code to come closer to the true distribution.

Environment Description: 
Goal Description: 

Here is a solution you came up with before. 

```
from __future__ import annotations

import copy
import enum
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

from uncertain_worms.structs import (
    Environment,
    Heuristic,
    InitialModel,
    Observation,
    ObservationModel,
    RewardModel,
    State,
    TransitionModel,
)



# -----------------------------------------------------------------------------#
# Domain parameters (feel free to tweak)                                       #
# -----------------------------------------------------------------------------#
NUM_ROCKS   = 2
GRID_SIZE   = 5
ROCK_POSITIONS = [(1, 1), (1, 4)]   # len == NUM_ROCKS

# -----------------------------------------------------------------------------#
# Actions                                                                      #
# -----------------------------------------------------------------------------#
class RockSampleActions(enum.IntEnum):
    """They can be added directly to the state position."""
    MOVE_NORTH  = 0
    MOVE_SOUTH  = 1
    MOVE_EAST   = 2
    MOVE_WEST   = 3
    SAMPLE      = 4
    EXIT        = 5

    CHECK_ROCK_0 = 6
    CHECK_ROCK_1 = 7


CHECK_ACTIONS: List[int] = [
    RockSampleActions.CHECK_ROCK_0,
    RockSampleActions.CHECK_ROCK_1,
]

# -----------------------------------------------------------------------------#
# Observation                                                                  #
# -----------------------------------------------------------------------------#
@dataclass
class RockSampleObservation(Observation):
    """Observation after an action.

    Always embeds the rover pose (x, y).  
    For sensor actions:
        * ``rock_idx``  – index of inspected rock
        * ``is_good``   – noisy reading (True = GOOD, False = BAD)
    For all other actions both fields are ``None``.
    """
    x: int
    y: int
    rock_idx: int | None
    is_good: bool | None

# -----------------------------------------------------------------------------#
# State                                                                        #
# -----------------------------------------------------------------------------#
@dataclass
class RockSampleState(State):
    """Full underlying state (fully observable to the simulator)."""
    x: int
    y: int
    rocks: Tuple[bool, ...]   # immutable tuple of good/bad flags

    # --- Equality / hashing --------------------------------------------------
    def __eq__(self, other: object) -> bool:                  # type: ignore[override]
        return (
            isinstance(other, RockSampleState)
            and self.x == other.x
            and self.y == other.y
            and self.rocks == other.rocks
        )

    # Convenience -------------------------------------------------------------
    def at_rock(self) -> int | None:
        """Return the index of the rock at the agent's (x,y) or ``None``."""
        try:
            return ROCK_POSITIONS.index((self.x, self.y))
        except ValueError:
            return None



def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Reward for sampling a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Sampling a good rock gives a positive reward
            else:
                reward = -10.0  # Sampling a bad rock gives a negative reward
        else:
            reward = -10.0  # Sampling without being at a rock gives a negative reward
    # Reward for checking a rock
    elif action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        if rock_idx is not None and rock_idx == (action - RockSampleActions.CHECK_ROCK_0):
            reward = 0.0  # Checking the correct rock gives no reward
        else:
            reward = -1.0  # Checking without being at the rock or the wrong rock gives a small negative reward
    else:
        reward = 0.0  # No reward for other actions

    # Check if the episode is done
    if action == RockSampleActions.EXIT:
        # Episode is done only if the agent is at a specific exit location (e.g., (0, 0))
        if (state.x, state.y) == (0, 0):
            done = True
        else:
            done = False
    else:
        done = False

    return reward, done

```


Here are some samples from the real world that were impossible under your model
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[G,B])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[G,B])
Output float: -1.0
Output bool: False



And here are some samples from your code under the same conditions
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[G,B])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[G,B])
Output float: 0.0
Output bool: False



Here are some samples from the real world that were impossible under your model
Input RockSampleState: RockSampleState(x=4, y=1, rocks=[B,B])
Input int: 5
Input RockSampleState: RockSampleState(x=4, y=1, rocks=[B,B])
Output float: 10.0
Output bool: True



And here are some samples from your code under the same conditions
Input RockSampleState: RockSampleState(x=4, y=1, rocks=[B,B])
Input int: 5
Input RockSampleState: RockSampleState(x=4, y=1, rocks=[B,B])
Output float: 0.0
Output bool: False



Here are some samples from the real world that were impossible under your model
Input RockSampleState: RockSampleState(x=1, y=4, rocks=[B,G])
Input int: 7
Input RockSampleState: RockSampleState(x=1, y=4, rocks=[B,G])
Output float: -1.0
Output bool: False



And here are some samples from your code under the same conditions
Input RockSampleState: RockSampleState(x=1, y=4, rocks=[B,G])
Input int: 7
Input RockSampleState: RockSampleState(x=1, y=4, rocks=[B,G])
Output float: 0.0
Output bool: False



Here are some samples from the real world that were impossible under your model
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,B])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,B])
Output float: -1.0
Output bool: False



And here are some samples from your code under the same conditions
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,B])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,B])
Output float: 0.0
Output bool: False



Here are some samples from the real world that were impossible under your model
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,G])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,G])
Output float: -1.0
Output bool: False



And here are some samples from your code under the same conditions
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,G])
Input int: 6
Input RockSampleState: RockSampleState(x=1, y=1, rocks=[B,G])
Output float: 0.0
Output bool: False





Explain what you believe is the reward conditioned on state, action, next state transition in english, then improve your code to better model the true distribution.

Please implement the code for the following the template. 
You must implement the ‘ reward_func ‘ function. 

The code needs to be directly runnable with (state, action, next_state) and return (reward, done). 

Do not output examples. 
Do not create any new classes. 
Do not rewrite existing classes.
Do not import any new modules from anywhere.
Do not list out specific indices that overfit to the examples, but include ranges.
Put the ‘ reward_func ‘ function in a python code block.
#define assistant
Certainly! Based on the provided samples and the discrepancies between the real-world data and the generated model, it appears that the reward function is not correctly handling the conditions for checking rocks and exiting the environment.

### Explanation of the Reward Function:

1. **Sampling a Rock (ACTION: 4)**:
   - If the agent is at a rock and samples it, the reward is +10 if the rock is good and -10 if the rock is bad.
   - If the agent attempts to sample without being at a rock, the reward is -10.

2. **Checking a Rock (ACTIONS: 6, 7)**:
   - If the agent is at the rock and checks it correctly, the reward is 0.
   - If the agent checks a rock but is not at the correct rock or at no rock, the reward is -1.

3. **Exiting the Environment (ACTION: 5)**:
   - If the agent is at the exit location (0, 0) and exits, the reward is +10 and the episode is done.
   - If the agent attempts to exit from any other location, the reward is 0 and the episode is not done.

### Improved Code:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Reward for sampling a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Sampling a good rock gives a positive reward
            else:
                reward = -10.0  # Sampling a bad rock gives a negative reward
        else:
            reward = -10.0  # Sampling without being at a rock gives a negative reward
    # Reward for checking a rock
    elif action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        correct_rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if rock_idx == correct_rock_idx:
            reward = 0.0  # Checking the correct rock gives no reward
        else:
            reward = -1.0  # Checking without being at the rock or the wrong rock gives a small negative reward
    # Reward for exiting the environment
    elif action == RockSampleActions.EXIT:
        if (state.x, state.y) == (0, 0):
            reward = 10.0  # Exiting from the exit location gives a positive reward
            done = True
        else:
            reward = 0.0  # Exiting from any other location gives no reward
            done = False
    else:
        reward = 0.0  # No reward for other actions
        done = False

    return reward, done
```

### Explanation of Changes:
1. **Checking a Rock**:
   - The code now correctly checks if the agent is at the correct rock before applying the reward. If the agent is not at the correct rock, a penalty of -1 is applied.

2. **Exiting the Environment**:
   - The code now correctly applies a reward of +10 and sets `done` to `True` only if the agent is at the exit location (0, 0). Otherwise, the reward is 0 and `done` is `False`.

These changes should align the model more closely with the observed real-world data.
#define assistant


def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Reward for sampling a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Sampling a good rock gives a positive reward
            else:
                reward = -10.0  # Sampling a bad rock gives a negative reward
        else:
            reward = -10.0  # Sampling without being at a rock gives a negative reward
    # Reward for checking a rock
    elif action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        correct_rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if rock_idx == correct_rock_idx:
            reward = 0.0  # Checking the correct rock gives no reward
        else:
            reward = -1.0  # Checking without being at the rock or the wrong rock gives a small negative reward
    # Reward for exiting the environment
    elif action == RockSampleActions.EXIT:
        if (state.x, state.y) == (0, 0):
            reward = 10.0  # Exiting from the exit location gives a positive reward
            done = True
        else:
            reward = 0.0  # Exiting from any other location gives no reward
            done = False
    else:
        reward = 0.0  # No reward for other actions
        done = False

    return reward, done

#define user
Traceback (most recent call last):
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/partially_obs_planning_agent.py", line 280, in evaluate_reward_model
    pred_reward, pred_terminated = self.planner.reward_model(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/base_policy.py", line 107, in reward_model
    reward, done = locals()[func_name](
                   ^^^^^^^^^^^^^^^^^^^^
  File "generated_code_reward_func400.py", line 43, in reward_func
    return reward, done
                   ^^^^
UnboundLocalError: cannot access local variable 'done' where it is not associated with a value

Traceback (most recent call last):
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/partially_obs_planning_agent.py", line 280, in evaluate_reward_model
    pred_reward, pred_terminated = self.planner.reward_model(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/base_policy.py", line 107, in reward_model
    reward, done = locals()[func_name](
                   ^^^^^^^^^^^^^^^^^^^^
  File "generated_code_reward_func400.py", line 43, in reward_func
    return reward, done
                   ^^^^
UnboundLocalError: cannot access local variable 'done' where it is not associated with a value

