Certainly! Based on the provided samples, it seems there are a few issues with the current `reward_func` implementation. Specifically:

1. The reward for checking a rock when the agent is at the correct rock position should be `-1.0`, even if the rock is bad.
2. The reward for exiting the grid from any position (not just the top-left corner) should be `10.0` and mark the episode as done.

Let's update the `reward_func` to address these issues:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    # Check if the agent is at a rock position
    at_rock = state.at_rock()

    # Determine the reward based on the action
    if action == RockSampleActions.SAMPLE:
        if at_rock is not None:
            if state.rocks[at_rock]:  # If the rock is good
                reward = 10.0
            else:  # If the rock is bad
                reward = -10.0
        else:
            reward = -10.0  # Sampling when not at a rock

    elif action in CHECK_ACTIONS:
        if at_rock == action - RockSampleActions.CHECK_ROCK_0:
            reward = -1.0  # Check rock when at the correct rock position (even if the rock is bad)
        else:
            reward = 0.0  # Check rock when not at the correct rock position

    elif action == RockSampleActions.EXIT:
        reward = 10.0  # Exit from any position
        done = True  # Episode is done

    return reward, done
```

This updated `reward_func` should better match the true distribution of rewards and episode termination conditions based on the provided samples.