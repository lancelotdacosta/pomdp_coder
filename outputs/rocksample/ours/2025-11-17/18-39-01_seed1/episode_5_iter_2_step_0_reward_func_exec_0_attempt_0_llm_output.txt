Certainly! Based on the provided samples and the goal of improving the `reward_func` to better match the real-world distribution, let's analyze the reward conditions:

1. **Sampling Action**:
   - If the agent is at a rock position and performs the `SAMPLE` action:
     - The reward is +10 if the rock is good.
     - The reward is -10 if the rock is bad.
   - If the agent is not at a rock position and performs the `SAMPLE` action, the reward is -10.

2. **Check Rock Actions**:
   - Performing a `CHECK_ROCK` action when the agent is at the corresponding rock position results in a reward of -1.
   - Performing a `CHECK_ROCK` action when the agent is not at the corresponding rock position results in a reward of 0.

3. **Exit Action**:
   - Performing the `EXIT` action:
     - The reward is +10 if the agent is at the exit position (the top-left corner of the grid, i.e., (0, 0)).
     - The reward is 0 otherwise.
   - The episode is marked as done after the `EXIT` action.

4. **Other Actions**:
   - All other actions (move actions) result in a reward of 0 and do not mark the episode as done.

Now, let's improve the `reward_func` function to better model these conditions:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    # Check if the agent is at a rock position
    at_rock = state.at_rock()

    # Determine the reward based on the action
    if action == RockSampleActions.SAMPLE:
        if at_rock is not None:
            if state.rocks[at_rock]:  # If the rock is good
                reward = 10.0
            else:  # If the rock is bad
                reward = -10.0
        else:
            reward = -10.0  # Sampling when not at a rock

    elif action in CHECK_ACTIONS:
        if at_rock == action - RockSampleActions.CHECK_ROCK_0:
            reward = -1.0  # Check rock when at the correct rock position
        else:
            reward = 0.0  # Check rock when not at the correct rock position

    elif action == RockSampleActions.EXIT:
        if state.x == 0 and state.y == 0:  # Exit from the top-left corner
            reward = 10.0
        else:
            reward = 0.0
        done = True

    return reward, done
```

This updated `reward_func` function should better match the observed real-world distribution by correctly handling the different actions and their corresponding rewards and episode termination conditions.