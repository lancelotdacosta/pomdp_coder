Based on the provided samples, it seems that the reward function should consider the following conditions:

1. **Exiting the Environment:**
   - If the agent is at position `(GRID_SIZE, 1)` and performs the `EXIT` action, the episode ends, and the reward is `10.0` if all rocks are bad (`False`).
   - If the agent performs the `EXIT` action from any other position, the episode does not end, and the reward is `0.0`.

2. **Sampling a Rock:**
   - If the agent is at a rock position and performs the `SAMPLE` action, the reward is `-1.0` if the rock is bad (`False`), and `-10.0` if the rock is good (`True`).
   - If the agent performs the `SAMPLE` action when not at a rock position, the reward is `0.0`.

3. **Checking a Rock:**
   - Checking a rock status (actions `CHECK_ROCK_0` and `CHECK_ROCK_1`) always gives a reward of `-1.0`.

4. **Moving Actions:**
   - Moving actions (`MOVE_NORTH`, `MOVE_SOUTH`, `MOVE_EAST`, `MOVE_WEST`) give no reward (`0.0`).

Here is the improved `reward_func` function that better models the true distribution:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Reward and done flags
    reward = 0.0
    done = False
    
    # Check if the agent is at a rock position
    rock_idx = state.at_rock()
    
    # Define rewards based on actions
    if action == RockSampleActions.SAMPLE:
        if rock_idx is not None:
            # Sampling a rock gives -10.0 reward if it's good, otherwise -1.0
            if state.rocks[rock_idx]:
                reward = -10.0
            else:
                reward = -1.0
        else:
            # Sampling when not at a rock position gives no reward
            reward = 0.0
    elif action == RockSampleActions.EXIT:
        # Exiting the environment gives a positive reward and ends the episode
        if (state.x, state.y) == (GRID_SIZE, 1):
            if all(not rock for rock in state.rocks):
                reward = 10.0
            else:
                reward = 0.0
            done = True
        else:
            reward = 0.0
    elif action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        # Moving actions give no reward
        reward = 0.0
    elif action in CHECK_ACTIONS:
        # Checking rock status gives a reward of -1.0
        reward = -1.0
    
    return reward, done
```

This function should now better align with the observed distribution from the real world. The key changes are:

- **Sampling a rock** now correctly gives a reward of `-10.0` if the rock is good (`True`) and `-1.0` if the rock is bad (`False`).
- **Exiting the environment** only ends the episode and gives a positive reward if the agent is at position `(GRID_SIZE, 1)` and all rocks are bad (`False`).