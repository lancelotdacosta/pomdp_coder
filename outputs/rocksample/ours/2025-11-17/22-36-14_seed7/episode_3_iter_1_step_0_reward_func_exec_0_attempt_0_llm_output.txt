Certainly! Let's start by explaining the reward conditioned on state, action, and next state transition in English:

1. **Exit Action (Action 5)**:
   - If the agent performs the exit action (`EXIT`), the episode ends with a reward of 0.0 if the agent is at the exit position (which is typically not defined in your environment). If the exit action is taken at any other position, it should still end the episode but might not give a reward.

2. **Sample Action (Action 4)**:
   - If the agent samples a rock, it must be at a rock position.
   - If the rock is good (`True`), the agent receives a positive reward of 10.0.
   - If the rock is bad (`False`), the agent receives a negative reward of -10.0.
   - If the agent samples without being at a rock position, it receives a negative reward of -1.0.

3. **Check Rock Actions (Actions 6 and 7)**:
   - Checking a rock does not provide an immediate reward. The action is used to gather information about the rock's quality.

4. **Movement Actions (Actions 0, 1, 2, 3)**:
   - Movement actions do not provide any immediate reward.

Now, let's improve the `reward_func` function to better match the true distribution:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define the reward based on the action and the resulting state
    if action == RockSampleActions.EXIT:
        # Exiting the environment generally ends the episode
        return 0.0, True

    elif action == RockSampleActions.SAMPLE:
        # If the agent samples a rock, check if it's at a rock position
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # If the rock is good, give a positive reward
            if state.rocks[rock_idx]:
                return 10.0, False
            else:
                # If the rock is bad, give a negative reward
                return -10.0, False
        else:
            # Sampling without being at a rock position gives a negative reward
            return -1.0, False

    elif action in CHECK_ACTIONS:
        # Checking a rock doesn't give an immediate reward
        return 0.0, False

    elif action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        # Movement actions generally don't give a reward unless they move to a specific rock and sample it
        return 0.0, False

    else:
        # Any other action not defined above
        return 0.0, False
```

### Explanation of Changes:

1. **Exit Action**:
   - The exit action now correctly ends the episode with a reward of 0.0 regardless of the agent's position.

2. **Sample Action**:
   - The `rock_idx` is checked against the current state's rock positions.
   - If the agent samples a good rock, it receives a positive reward of 10.0.
   - If the agent samples a bad rock, it receives a negative reward of -10.0.
   - If the agent samples without being at a rock position, it receives a negative reward of -1.0.

3. **Check Rock Actions**:
   - No immediate reward is given for checking rocks.

4. **Movement Actions**:
   - No immediate reward is given for movement actions.

These changes should better align the reward function with the observed behavior in the real world.