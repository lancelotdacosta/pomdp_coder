Certainly! Let's analyze the problem and then improve the `reward_func` function to better model the true reward distribution.

### Explanation of the Reward Function

1. **Exiting the Grid**:
   - The agent receives a reward of 10.0 if it successfully exits the grid by moving to the position (0, 0) and performs the EXIT action.
   - If the agent is not at (0, 0) when performing the EXIT action, the reward is 0.0.

2. **Sampling a Rock**:
   - If the agent samples a rock while standing on a rock's position, it receives a reward of 10.0 if the rock is good and -10.0 if the rock is bad.
   - If the agent samples a rock but is not standing on a rock's position, it receives a reward of -10.0.

3. **Checking a Rock**:
   - The agent can perform CHECK_ROCK actions to inspect rocks. These actions do not move the agent but provide a noisy observation of the rock's quality.
   - Checking a rock should incur a small penalty (e.g., -1.0) to avoid overuse of this action.

4. **Other Actions**:
   - All other actions (e.g., moving) do not provide any immediate reward.

5. **Episode Termination**:
   - The episode is considered done if the agent exits the grid by returning to the position (0, 0).

### Improved `reward_func` Function

Here is the improved `reward_func` function:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define the reward for each type of action
    def get_reward_for_action(action, state, next_state):
        if action == RockSampleActions.EXIT:
            # Reward for exiting the grid
            return 10.0 if next_state.x == 0 and next_state.y == 0 else 0.0
        elif action == RockSampleActions.SAMPLE:
            # Reward for sampling a rock
            rock_idx = state.at_rock()
            if rock_idx is not None:
                return 10.0 if state.rocks[rock_idx] else -10.0
            else:
                return -10.0
        elif action in CHECK_ACTIONS:
            # Small penalty for checking a rock
            return -1.0
        else:
            # No reward for other actions
            return 0.0

    # Determine if the episode is done
    def is_done(next_state):
        # Episode is done if the agent exits the grid (returns to (0, 0))
        return next_state.x == 0 and next_state.y == 0

    reward = get_reward_for_action(action, state, next_state)
    done = is_done(next_state)

    return reward, done
```

### Explanation of Changes

1. **EXIT Action**:
   - The reward for the EXIT action is now correctly calculated based on whether the agent is at position (0, 0) in the next state.

2. **SAMPLE Action**:
   - The reward for sampling a rock is correctly calculated based on the quality of the rock at the agent's current position.

3. **CHECK_ROCK Actions**:
   - Added a small penalty (-1.0) for performing CHECK_ROCK actions to discourage excessive use.

4. **Other Actions**:
   - All other actions do not provide any immediate reward, as before.

5. **Episode Termination**:
   - The episode is considered done if the agent exits the grid by returning to the position (0, 0).

This should better align with the true distribution of rewards in the environment.