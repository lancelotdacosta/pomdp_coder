Certainly! Let's break down the reward function logic first:

1. **Exit Action**: The agent receives a reward of 10.0 if it exits from one of the valid exit points. Otherwise, there is no reward, and the episode is not considered done.
2. **Sample Action**: If the agent samples a rock, it should receive a reward of 10.0 for sampling a good rock and a penalty of -10.0 for sampling a bad rock. This should only happen if the agent is at a rock's position.
3. **Check Actions**: These actions might have a small penalty of -1.0.
4. **Movement Actions**: There is no specific reward for movement actions.

The issue in your current implementation is that the `reward_func` does not correctly handle the case where the agent tries to exit from a position that is not a valid exit point. Specifically, it incorrectly assigns a reward of 10.0 and sets `done` to `True` even when the agent is not at a valid exit point.

Here is the improved `reward_func`:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Define the reward and done flags
    reward = 0.0
    done = False

    # Check if the agent has taken the exit action
    if action == RockSampleActions.EXIT:
        # Define the valid exit points
        VALID_EXIT_POINTS = [(0, 0), (4, 2), (4, 1), (4, 3), (4, 4)]
        if (state.x, state.y) in VALID_EXIT_POINTS:
            done = True
            reward = 10.0  # Reward for successfully exiting
        else:
            reward = 0.0  # No reward for exiting from the wrong location
            done = False  # Episode is not done

    # Check if the agent has sampled a rock
    elif action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Reward for sampling a good rock
            else:
                reward = -10.0  # Penalty for sampling a bad rock

    # Check if the agent has taken a check action
    elif action in CHECK_ACTIONS:
        # Check actions might have a small penalty
        reward = -1.0

    # No specific reward for movement actions
    return reward, done
```

### Explanation of Changes:
1. **Exit Action**:
   - Added a check to ensure that the episode is not marked as done (`done = False`) if the agent tries to exit from an invalid location.
   - Corrected the reward assignment for invalid exit points to 0.0.

2. **Sample Action**:
   - The logic for sampling a rock remains unchanged.

3. **Check Actions**:
   - The logic for assigning a small penalty (-1.0) for check actions remains unchanged.

4. **Movement Actions**:
   - No specific reward for movement actions, which is already handled correctly in the original code.

This should better model the true distribution of rewards and episode termination conditions.