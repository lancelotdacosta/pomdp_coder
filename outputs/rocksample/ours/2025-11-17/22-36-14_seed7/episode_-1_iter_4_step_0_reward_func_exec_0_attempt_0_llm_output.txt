Certainly! Let's first understand the reward conditioned on state, action, and next state transition:

1. **Movement Actions**:
   - No reward is given for movement actions (MOVE_NORTH, MOVE_SOUTH, MOVE_EAST, MOVE_WEST).

2. **Sampling Action (SAMPLE)**:
   - If the agent is at a rock position and samples it:
     - If the rock type changes (i.e., the rock was sampled successfully), the reward is +10.
     - If the rock type does not change (i.e., the sample failed), the reward is -1.
   - If the agent samples at a location with no rock, the reward is -10.

3. **Exit Action (EXIT)**:
   - If the agent exits the grid from the boundary, the reward is +10 and the episode ends.
   - If the agent exits from any other location, the reward is 0.

4. **Check Actions (CHECK_ROCK_0, CHECK_ROCK_1)**:
   - If the agent checks a rock at the correct position, the reward is 0.
   - If the agent checks a rock at an incorrect position, the reward is -1.

Now, let's improve the `reward_func` function to better model the true distribution:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    # Movement actions do not give rewards
    if action in [RockSampleActions.MOVE_NORTH, RockSampleActions.MOVE_SOUTH, RockSampleActions.MOVE_EAST, RockSampleActions.MOVE_WEST]:
        reward = 0.0

    # Sampling action
    elif action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if next_state.rocks[rock_idx] != state.rocks[rock_idx]:
                # The rock was sampled and the type changed
                reward = 10.0
            else:
                # The rock was not changed (bad sample)
                reward = -1.0
        else:
            # Sampled at a location with no rock
            reward = -10.0

    # Exit action
    elif action == RockSampleActions.EXIT:
        # Check if the agent is at the boundary of the grid
        if state.x == 0 or state.x == GRID_SIZE - 1 or state.y == 0 or state.y == GRID_SIZE - 1:
            reward = 10.0
            done = True
        else:
            reward = 0.0

    # Check actions
    elif action in CHECK_ACTIONS:
        rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if state.at_rock() == rock_idx:
            # At the correct rock position
            reward = 0.0
        else:
            # Not at the correct rock position
            reward = -1.0

    return reward, done
```

### Explanation of Changes:
1. **Check Actions**:
   - The previous implementation gave a reward of 0 for check actions even when the agent was not at the correct rock position. This has been corrected to give a reward of -1 when the agent is not at the correct rock position.

2. **Sampling Action**:
   - The previous implementation gave a reward of -1 for sampling at a location with no rock, which was incorrect. It should have given a reward of -10. This has been corrected.

3. **Exit Action**:
   - The previous implementation correctly gave a reward of 10 and marked the episode as done if the agent exited from the boundary. However, it incorrectly gave a reward of 10 when the agent was at the boundary but did not exit. This has been corrected to give a reward of 0 if the agent is not at the boundary.

These changes ensure that the reward function better aligns with the observed distribution and the intended behavior of the environment.