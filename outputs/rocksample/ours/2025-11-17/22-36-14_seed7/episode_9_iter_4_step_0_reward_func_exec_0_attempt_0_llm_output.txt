The discrepancy in the reward function arises because the condition for the episode being done is not correctly handled. The real-world samples suggest that the episode is done if the agent exits the grid by returning to the starting position (0, 0), regardless of whether it is already at (0, 0) or another position. Additionally, the reward for exiting the grid should only be given if the agent successfully returns to (0, 0).

Here's the improved `reward_func` function:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define the reward for each type of action
    def get_reward_for_action(action, state, next_state):
        if action == RockSampleActions.EXIT:
            # Reward for exiting the grid
            return 10.0 if next_state.x == 0 and next_state.y == 0 else 0.0
        elif action == RockSampleActions.SAMPLE:
            # Reward for sampling a rock
            rock_idx = state.at_rock()
            if rock_idx is not None:
                return 10.0 if state.rocks[rock_idx] else -10.0
            else:
                return -10.0
        elif action in CHECK_ACTIONS:
            # Small penalty for checking a rock
            return -1.0
        else:
            # No reward for other actions
            return 0.0

    # Determine if the episode is done
    def is_done(next_state):
        # Episode is done if the agent exits the grid (returns to (0, 0))
        return next_state.x == 0 and next_state.y == 0

    reward = get_reward_for_action(action, state, next_state)
    done = is_done(next_state)

    return reward, done
```

### Explanation:
1. **Reward for Exiting the Grid**:
   - The reward for exiting the grid (action `EXIT`) is 10.0 only if the agent's position in the next state is (0, 0). This ensures that the reward is given only when the agent successfully returns to the starting position.

2. **Episode Termination**:
   - The episode is marked as done if the agent's position in the next state is (0, 0), regardless of the action taken. This ensures that the episode ends correctly when the agent returns to the starting position.

By making these changes, the reward function should now better align with the observed behavior from the real world.