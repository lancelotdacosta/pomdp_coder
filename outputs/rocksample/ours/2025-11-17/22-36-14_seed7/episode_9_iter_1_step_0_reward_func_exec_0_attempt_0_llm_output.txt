Certainly! Let's break down the problem and then improve the `reward_func` function accordingly.

### Problem Analysis

1. **Action 7 (CHECK_ROCK_1)**:
   - The real world samples show a reward of `-1.0` for this action, regardless of the state. This suggests that there might be a penalty for checking a rock without sampling it.

2. **Action 5 (EXIT)**:
   - The real world samples show a reward of `10.0` when the agent exits the grid from any position, not just `(0, 0)`. This indicates that the exit action should always provide a reward of `10.0` and terminate the episode.

3. **Action 5 (EXIT) at (0, 0)**:
   - The real world samples show a reward of `0.0` and the episode not being done if the agent is already at `(0, 0)` and attempts to exit. This suggests that the episode should not be considered done if the agent is already at the exit point and tries to exit again.

### Improved Reward Function

Based on the above analysis, let's update the `reward_func` function to better match the observed distribution.

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Define the reward for each type of action
    def get_reward_for_action(action, state, next_state):
        if action == RockSampleActions.EXIT:
            # Reward for exiting the grid
            return 10.0 if (next_state.x != 0 or next_state.y != 0) else 0.0
        elif action == RockSampleActions.SAMPLE:
            # Reward for sampling a rock
            rock_idx = state.at_rock()
            if rock_idx is not None:
                return 10.0 if state.rocks[rock_idx] else -10.0
            else:
                return -10.0
        elif action in CHECK_ACTIONS:
            # Penalty for checking a rock without sampling it
            return -1.0
        else:
            # No reward for other actions
            return 0.0

    # Determine if the episode is done
    def is_done(next_state):
        # Episode is done if the agent exits the grid (returns to a position other than (0, 0))
        return next_state.x != 0 or next_state.y != 0

    reward = get_reward_for_action(action, state, next_state)
    done = is_done(next_state)

    return reward, done
```

### Explanation

1. **Action 7 (CHECK_ROCK_1)**:
   - The function now returns a reward of `-1.0` for any check action, regardless of the state or the next state.

2. **Action 5 (EXIT)**:
   - The function now returns a reward of `10.0` if the agent exits from any position other than `(0, 0)`. If the agent is already at `(0, 0)` and tries to exit, the reward is `0.0`.

3. **Action 5 (EXIT) at (0, 0)**:
   - The episode is not considered done if the agent is already at `(0, 0)` and tries to exit again.

This should better align with the observed distribution and handle the discrepancies in the provided samples.