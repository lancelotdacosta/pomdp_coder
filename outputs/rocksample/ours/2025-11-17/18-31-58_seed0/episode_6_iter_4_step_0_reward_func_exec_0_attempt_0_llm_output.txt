Certainly! Let's analyze the discrepancies and improve the `reward_func` function accordingly.

### Analysis

1. **Exit Action Discrepancy**:
   - The exit action should provide a reward of 10.0 and terminate the episode if the agent is at the bottom-right corner (GRID_SIZE - 1, GRID_SIZE - 1). However, the provided samples show that the exit action should also provide a reward of 10.0 and terminate the episode if the agent is at any position on the rightmost column (x = GRID_SIZE - 1).

2. **Check Rock Action Discrepancy**:
   - The check rock actions should provide a small penalty of -1.0 regardless of whether the agent is at the rock's position or not. This is different from the current implementation where the penalty is only applied if the agent is at the rock's position and performs the correct check action.

### Improved `reward_func` Function

Here is the improved `reward_func` function:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Check if the agent has taken the exit action
    if action == RockSampleActions.EXIT:
        if state.x == GRID_SIZE - 1:  # Exit can be taken from any position on the rightmost column
            return 10.0, True
        else:
            return 0.0, False
    
    # Check if the agent has sampled a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                return 10.0, False  # Sampling a good rock gives a positive reward
            else:
                return -10.0, False  # Sampling a bad rock gives a negative reward
        else:
            return 0.0, False  # No rock at this position, no reward
    
    # Check if the agent has checked a rock
    if action in CHECK_ACTIONS:
        return -1.0, False  # Small penalty for checking a rock, regardless of position or action correctness
    
    # For all other actions (move), there is no reward and the episode is not done
    return 0.0, False
```

### Explanation

1. **Exit Action**:
   - The exit action now checks if the agent's x-coordinate is on the rightmost column (x == GRID_SIZE - 1). If so, it returns a reward of 10.0 and sets `done` to True, otherwise, it returns 0.0 and `done` to False.

2. **Check Rock Action**:
   - The check rock actions now always provide a small penalty of -1.0, regardless of the agent's position or the specific check action performed. This aligns with the observed distribution from the real world.

3. **Sample Action**:
   - The sample action remains unchanged, as it correctly provides rewards based on the rock's quality and the agent's position.

This should bring the generated model closer to the true distribution observed in the real world.