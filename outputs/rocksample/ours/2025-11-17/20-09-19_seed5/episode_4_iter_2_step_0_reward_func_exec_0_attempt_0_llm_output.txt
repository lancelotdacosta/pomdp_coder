Based on the provided samples, it seems that the real-world environment imposes a small negative reward for certain actions, likely to penalize unnecessary or ineffective actions. Specifically, the `-1.0` reward appears to be given when the agent performs a `CHECK_ROCK` action while already knowing the state of the rock. This suggests that the agent should only perform `CHECK_ROCK` actions when it is uncertain about the rock's state.

Let's update the `reward_func` to incorporate this behavior:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Define the reward and done flags
    reward = 0.0
    done = False

    # Check if the action is to exit
    if action == RockSampleActions.EXIT:
        done = True
        # Reward for exiting the environment
        reward = 10.0 if all(next_state.rocks) else -10.0
    elif action == RockSampleActions.SAMPLE:
        # Check if the agent is at a rock position
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # Sample the rock at the current position
            if next_state.rocks[rock_idx]:
                reward = 10.0  # Good rock
            else:
                reward = -10.0  # Bad rock
        else:
            reward = -10.0  # Sampling when not at a rock position
    elif action in CHECK_ACTIONS:
        rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if (state.x, state.y) == ROCK_POSITIONS[rock_idx]:
            # If the agent is already at the rock and checks it
            if state.rocks[rock_idx] == next_state.rocks[rock_idx]:
                reward = -1.0  # Small penalty for redundant check
            else:
                reward = 0.0  # No penalty if the check changes the rock state
        else:
            reward = -1.0  # Penalty for checking a rock when not at its position

    return reward, done
```

### Explanation:
1. **Exit Action**:
   - The reward is `10.0` if all rocks are good (`True`), otherwise `-10.0`.
   - The episode ends (`done = True`).

2. **Sample Action**:
   - If the agent is at a rock position, it samples the rock.
   - If the rock is good (`True`), the reward is `10.0`.
   - If the rock is bad (`False`), the reward is `-10.0`.
   - If the agent tries to sample without being at a rock position, the reward is `-10.0`.

3. **Check Rock Actions**:
   - For actions `CHECK_ROCK_0` and `CHECK_ROCK_1`, the function determines the rock index.
   - If the agent is at the rock's position and the state of the rock does not change, a small penalty of `-1.0` is given.
   - If the agent is not at the rock's position, a small penalty of `-1.0` is given.
   - No penalty if the check changes the rock state.

This updated `reward_func` should better match the observed distribution from the real-world samples.