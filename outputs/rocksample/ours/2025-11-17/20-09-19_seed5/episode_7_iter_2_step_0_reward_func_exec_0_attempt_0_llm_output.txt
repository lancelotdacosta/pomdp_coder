To improve the `reward_func` function, we need to ensure it correctly models the reward based on the state, action, and next state transitions. Based on the provided samples, here is the reasoning for the reward and done conditions:

1. **Exit Action**:
   - If the agent takes the `EXIT` action, the episode should terminate (`done = True`). The reward is `0.0`.

2. **Sample Action**:
   - If the agent takes the `SAMPLE` action at a rock position:
     - If the rock is good, the reward is `1.0`.
     - If the rock is bad, the reward is `-1.0`.
   - If the agent takes the `SAMPLE` action but is not at a rock position, the reward is `-1.0`.

3. **Check Rock Actions**:
   - For actions `CHECK_ROCK_0` and `CHECK_ROCK_1`, the reward should be `0.0` if the agent is not at the corresponding rock position. If the agent is at the rock position, the reward should be `-1.0`.

4. **Other Actions**:
   - For all other actions (move, check), the reward should be `0.0`.

Here is the improved `reward_func` function:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    
    # Reward and done flags
    reward = 0.0
    done = False
    
    # Check if the agent has taken the exit action
    if action == RockSampleActions.EXIT:
        done = True
        return reward, done

    # Check if the agent has sampled a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # If the agent samples a good rock, give a positive reward
            if next_state.rocks[rock_idx]:
                reward = 1.0
            else:
                reward = -1.0
        else:
            # Penalize for sampling when not at a rock
            reward = -1.0
    
    # Check if the agent has taken a check action
    if action in CHECK_ACTIONS:
        rock_idx = state.at_rock()
        if rock_idx is not None and rock_idx == (action - RockSampleActions.CHECK_ROCK_0):
            # Penalty for checking a rock
            reward = -1.0
        else:
            # No penalty for checking when not at the rock
            reward = 0.0

    # No reward for other actions (move, check)
    return reward, done
```

This function should now better model the true distribution of rewards based on the provided samples. It correctly handles the exit action, sampling actions, and check rock actions, ensuring that the reward is appropriately assigned in each case.