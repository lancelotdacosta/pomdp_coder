Based on the provided samples and the discrepancies between the real-world rewards and the rewards generated by your current model, it seems that the reward function is more nuanced than initially implemented. Specifically, the action `EXIT` appears to only provide a positive reward and terminate the episode if the agent is at a specific position, likely the exit position.

Here's the improved `reward_func` function:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the current state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """

    # Define the reward and done flags
    reward = 0.0
    done = False
    
    # Check if the action is to sample a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            # If the agent is at a rock and samples it, get a reward based on the rock's quality
            if state.rocks[rock_idx]:
                reward = 10.0  # Good rock
            else:
                reward = -10.0  # Bad rock
    
    # Check if the action is to check a rock
    elif action in CHECK_ACTIONS:
        reward = -1.0  # Penalty for checking a rock
    
    # Check if the action is to exit the environment
    elif action == RockSampleActions.EXIT:
        # Only provide a positive reward and terminate if the agent is at a specific position (exit position)
        if (state.x, state.y) in [(0, 0), (4, 4)]:
            reward = 10.0  # Large positive reward for exiting
            done = True  # Episode ends when the agent exits
        else:
            reward = 0.0  # No reward if not at the exit position
            done = False  # Episode does not end

    return reward, done
```

### Explanation
- **Sampling Rocks**: The reward for sampling a rock is correctly determined by the quality of the rock. Good rocks give a positive reward of 10.0, while bad rocks give a negative reward of -10.0.
- **Checking Rocks**: Checking rocks still incurs a penalty of -1.0.
- **Exiting the Environment**: The key improvement is in the exit action. The reward and termination are only provided if the agent is at one of the predefined exit positions (e.g., (0, 0) or (4, 4)). If the agent is not at an exit position, the reward is 0.0 and the episode does not end.

This adjustment ensures that the model better aligns with the real-world samples provided.