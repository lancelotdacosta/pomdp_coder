Certainly! Based on the provided samples, it seems that the reward is conditioned on the agent's actions and the environment's state transitions. Here's a breakdown of the reward conditions:

1. **Exiting the Grid:**
   - The agent receives a reward of 10.0 if it performs the `EXIT` action (action 5) and is at the exit point (position (0, 0)).

2. **Checking Rocks:**
   - The agent receives a reward of -1.0 if it performs a `CHECK` action (actions 6 or 7) when it is at a rock position.
   - No reward is given for `CHECK` actions when not at a rock position.

3. **Sampling Rocks:**
   - The agent receives a reward of 10.0 if it performs the `SAMPLE` action (action 4) and is at a rock position where the rock is good (`True`).
   - No reward is given for `SAMPLE` actions when not at a rock position or when the rock is bad (`False`).

4. **Other Actions:**
   - No reward is given for other actions (e.g., moving).

Here is the improved `reward_func` function that better models the true distribution:

```python
def reward_func(state, action, next_state):
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    # Determine if the agent is at a rock position
    current_rock_idx = state.at_rock()
    next_rock_idx = next_state.at_rock()

    # Reward for exiting the grid (reaching the exit point)
    if action == RockSampleActions.EXIT and next_state.x == 0 and next_state.y == 0:
        return 10.0, True

    # Reward for checking a rock
    if action in CHECK_ACTIONS and current_rock_idx is not None:
        return -1.0, False

    # Reward for sampling a rock
    if action == RockSampleActions.SAMPLE:
        if current_rock_idx is not None and state.rocks[current_rock_idx]:
            return 10.0, True
        else:
            return 0.0, False

    # No reward for other actions
    return 0.0, False
```

This function should now handle the reward conditions as observed in the real world samples. The key changes include adding checks for `CHECK` actions and ensuring the agent only receives rewards when the conditions are met.