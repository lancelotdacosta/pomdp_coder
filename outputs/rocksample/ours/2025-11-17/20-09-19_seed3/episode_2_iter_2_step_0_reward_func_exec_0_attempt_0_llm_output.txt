Certainly! Based on the provided samples and the discrepancies between the real world and the generated model, it appears that the `reward_func` function is not correctly handling the conditions for the `EXIT` action. Specifically, the `EXIT` action should only terminate the episode if the agent is not at a rock, but the current implementation always sets `done` to `True` when the `EXIT` action is taken, regardless of the agent's position.

### Explanation in English

The reward function should:
1. **For the `EXIT` action**:
   - If the agent is not at a rock, the episode should end (`done = True`) with a reward of 0.
   - If the agent is at a rock, the episode should not end (`done = False`) with a reward of 0.

2. **For the `SAMPLE` action**:
   - If the agent is at a rock and samples a good rock, the reward should be 10.
   - If the agent is at a rock and samples a bad rock, the reward should be -10.
   - If the agent tries to sample when not at a rock, the reward should be -10.

3. **For the `CHECK` actions**:
   - If the agent performs a check action from the correct position, the reward should be 0.
   - If the agent performs a check action from an incorrect position, the reward should be -1.

4. **For all other actions**:
   - The reward should be 0 and `done` should be `False`.

### Improved `reward_func` Function

Here is the improved `reward_func` function:

```python
def reward_func(state: RockSampleState, action: int, next_state: RockSampleState) -> Tuple[float, bool]:
    """
    Args:
        state (RockSampleState): the state of the environment
        action (int): the action to be executed
        next_state (RockSampleState): the next state of the environment
    Returns:
        reward (float): the reward of that state
        done (bool): whether the episode is done
    """
    reward = 0.0
    done = False

    # Check if the agent has taken the exit action
    if action == RockSampleActions.EXIT:
        if state.at_rock() is None:
            done = True
        else:
            done = False
        return reward, done

    # Check if the agent has sampled a rock
    if action == RockSampleActions.SAMPLE:
        rock_idx = state.at_rock()
        if rock_idx is not None:
            if state.rocks[rock_idx]:
                reward = 10.0  # Reward for sampling a good rock
            else:
                reward = -10.0  # Penalty for sampling a bad rock
        else:
            reward = -10.0  # Penalty for trying to sample when not at a rock

    # Check if the agent has performed a check action
    if action in CHECK_ACTIONS:
        rock_idx = action - RockSampleActions.CHECK_ROCK_0
        if (state.x, state.y) != ROCK_POSITIONS[rock_idx]:
            reward = -1.0  # Penalty for checking a rock from an incorrect position

    # No other actions provide rewards in the given samples
    return reward, done
```

### Explanation of Changes

1. **EXIT Action**:
   - The condition `if state.at_rock() is None` ensures that the episode only ends if the agent is not at a rock.
   - If the agent is at a rock, `done` is set to `False`.

2. **SAMPLE Action**:
   - The logic for sampling a rock remains the same, but it is now more clearly separated from the `EXIT` action.

3. **CHECK Actions**:
   - The penalty for checking a rock from an incorrect position is applied as expected.

4. **Other Actions**:
   - No changes are made, as they do not provide rewards and do not end the episode.

This should better align with the real-world samples and correctly model the reward and termination conditions.