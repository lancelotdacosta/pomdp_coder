#define system
You are a robot exploring its environment. 

Environment Description: 
Goal Description: 

Your goal is to model the the distribution of next states conditioned on actions and previous states. 
You need to implement the python code to model the world, as seen in the provided experiences. 
Please follow the template to implement the code. 
The code needs to be directly runnable (state, action) and return a sample (next_state). 


Below are a few samples from the environment distribution. These are only samples from a larger distribution that your should model.

Input MinigridState: agent_pos=(np.int64(3), np.int64(5))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1, 10,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridState: agent_pos=(np.int64(3), np.int64(6))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1, 10,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]



Input MinigridState: agent_pos=(np.int64(7), np.int64(10))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridState: agent_pos=(np.int64(8), np.int64(10))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]



Input MinigridState: agent_pos=(np.int64(3), np.int64(9))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridState: agent_pos=(np.int64(3), np.int64(10))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]



Input MinigridState: agent_pos=(np.int64(3), np.int64(4))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridState: agent_pos=(np.int64(3), np.int64(5))
agent_dir=1
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1, 10,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]



Input MinigridState: agent_pos=(np.int64(5), np.int64(3))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2, 10,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]

Input Actions: 2
Output MinigridState: agent_pos=(np.int64(6), np.int64(3))
agent_dir=0
carrying=None
grid=[
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  1,  2,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2, 10,  1,  1,  1,  1,  1,  2, ],
[ 2,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2, ],
[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2, ],
]





Here is the template for the transition_func function. Please implement
the reward function following the template. The code needs to be directly
runnable.

‘‘‘
# type: ignore
from __future__ import annotations

from dataclasses import dataclass
from enum import IntEnum
from typing import Any, List, Optional, Tuple

import numpy as np
from numpy.typing import NDArray

AGENT_DIR_TO_STR = {0: ">", 1: "V", 2: "<", 3: "^"}
DIR_TO_VEC = [
    # Pointing right (positive X)
    np.array((1, 0)),
    # Down (positive Y)
    np.array((0, 1)),
    # Pointing left (negative X)
    np.array((-1, 0)),
    # Up (negative Y)
    np.array((0, -1)),
]

SEE_THROUGH_WALLS = True


class ObjectTypes(IntEnum):
    unseen = 0
    empty = 1
    wall = 2
    open_door = 4
    closed_door = 5
    locked_door = 6
    key = 7
    ball = 8
    box = 9
    goal = 10
    lava = 11
    agent = 12


class Direction(IntEnum):
    facing_right = 0
    facing_down = 1
    facing_left = 2
    facing_up = 3


class Actions(IntEnum):
    left = 0  # Turn left
    right = 1  # Turn right
    forward = 2  # Move forward
    pickup = 3  # Pick up an object
    drop = 4  # Drop an object
    toggle = 5  # Toggle/activate an object
    done = 6  # Done completing the task


@dataclass
class MinigridObservation(Observation):
    """
    Args:
        `image`: field of view in front of the agent.

        `agent_pos`: agent's position in the real world. It differs from the position
                     in the observation grid.
        `agent_dir`: agent's direction in the real world. It differs from the direction
                     of the agent in the observation grid.
        `carrying`: what the agent is carrying at the moment.
    """

    image: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int] = None


@dataclass
class MinigridState(State):
    """An agent exists in an indoor multi-room environment represented by a
    grid."""

    grid: NDArray[np.int8]
    agent_pos: Tuple[int, int]
    agent_dir: int
    carrying: Optional[int]

    @property
    def front_pos(self) -> Tuple[int, int]:
        """Get the position of the cell that is right in front of the agent."""

        return (
            np.array(self.agent_pos) + np.array(DIR_TO_VEC[self.agent_dir])
        ).tolist()

    @property
    def width(self) -> int:
        return self.grid.shape[0]

    @property
    def height(self) -> int:
        return self.grid.shape[1]

    def get_type_indices(self, type: int) -> List[Tuple[int, int]]:
        idxs = np.where(self.grid == type)  # Returns (row_indices, col_indices)
        return list(zip(idxs[0], idxs[1]))  # Combine row and column indices


    def get_field_of_view(self, view_size: int) -> NDArray[np.int8]:
        """Returns the field of view in front of the agent.

        DO NOT modify this function.
        """

        # Get the extents of the square set of tiles visible to the agent
        # Facing right
        if self.agent_dir == 0:
            topX = self.agent_pos[0]
            topY = self.agent_pos[1] - view_size // 2
        # Facing down
        elif self.agent_dir == 1:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1]
        # Facing left
        elif self.agent_dir == 2:
            topX = self.agent_pos[0] - view_size + 1
            topY = self.agent_pos[1] - view_size // 2
        # Facing up
        elif self.agent_dir == 3:
            topX = self.agent_pos[0] - view_size // 2
            topY = self.agent_pos[1] - view_size + 1
        else:
            assert False, "invalid agent direction"

        fov = np.full((view_size, view_size), ObjectTypes.wall, dtype=self.grid.dtype)

        # Compute the overlapping region in the grid.
        gx0 = max(topX, 0)
        gy0 = max(topY, 0)
        gx1 = min(topX + view_size, self.grid.shape[0])
        gy1 = min(topY + view_size, self.grid.shape[1])

        # Determine where the overlapping region goes in the padded array.
        px0 = max(0, -topX)
        py0 = max(0, -topY)

        # Copy the overlapping slice.
        fov[px0 : px0 + (gx1 - gx0), py0 : py0 + (gy1 - gy0)] = self.grid[
            gx0:gx1, gy0:gy1
        ]

        for _ in range(self.agent_dir + 1):
            # Rotate left
            fov = np.rot90(fov.T, k=1).T

        agent_pos = (self.grid.shape[0] // 2, self.grid.shape[1] - 1)
        self.grid[agent_pos] = ObjectTypes.agent

        return fov

def transition_func(state, action):
    """
    Args:
        state (MinigridState): the state of the environment 
        action (int): action to be taken in state `state`
    Returns:
        new_state (MinigridState): the new state of the environment
    """
    raise NotImplementedError
‘‘‘

Explain what you believe is the the distribution of next states conditioned on actions and previous states in english.
Additionally, please implement code to model the logic of the world. Please implement the 
code following the template. Only output the definition for ‘ transition_func ‘. 
You must implement the ‘ transition_func ‘ function.
Create any helper function inside the scope of ‘ transition_func ‘. 
Do not create any helper function outside the scope of ‘ transition_func ‘.
Do not output examples usage. 
Do not create any new classes.
Do not rewrite existing classes. 
Do not import any new modules from anywhere.
Do not overfit to the specific samples.
Put the ‘ transition_func ‘ function in a python code block.
#define assistant
```python
def transition_func(state, action):
    """
    Args:
        state (MinigridState): the state of the environment 
        action (int): action to be taken in state `state`
    Returns:
        new_state (MinigridState): the new state of the environment
    """

    def rotate Direction(agent_dir, action):
        if action == Actions.left:
            return (agent_dir - 1) % 4
        elif action == Actions.right:
            return (agent_dir + 1) % 4
        else:
            return agent_dir

    def move_forward(state):
        new_pos = np.array(state.agent_pos) + DIR_TO_VEC[state.agent_dir]
        new_pos = tuple(new_pos)
        
        if not (0 <= new_pos[0] < state.width and 0 <= new_pos[1] < state.height):
            # Out of bounds, stay in place
            return state.agent_pos, state.agent_dir
        
        if state.grid[new_pos] == ObjectTypes.wall:
            # Wall, stay in place
            return state.agent_pos, state.agent_dir
        
        return new_pos, state.agent_dir

    new_agent_pos = state.agent_pos
    new_agent_dir = state.agent_dir
    new_carrying = state.carrying

    if action == Actions.left or action == Actions.right:
        new_agent_dir = rotate_direction(state.agent_dir, action)
    elif action == Actions.forward:
        new_agent_pos, new_agent_dir = move_forward(state)
    # Other actions (pickup, drop, toggle, done) are not implemented for simplicity

    new_state = MinigridState(
        grid=state.grid.copy(),
        agent_pos=new_agent_pos,
        agent_dir=new_agent_dir,
        carrying=new_carrying
    )

    return new_state
```

### Explanation:
The `transition_func` function models the distribution of next states conditioned on actions and previous states in the following way:

1. **Direction Rotation**:
   - If the action is to turn left or right, the agent's direction is updated accordingly by rotating it 90 degrees in the respective direction.

2. **Move Forward**:
   - If the action is to move forward, the function calculates the new position by adding the direction vector to the current position.
   - If the new position is out of bounds or hits a wall, the agent stays in the same position.
   - Otherwise, the agent moves to the new position.

3. **Other Actions**:
   - For simplicity, actions like picking up, dropping, toggling, and completing the task are not implemented in this function. These actions would require additional logic to handle interactions with objects in the grid.

The function returns a new `MinigridState` object with the updated agent position, direction, and grid. The grid itself remains unchanged for now, but it could be modified if actions like picking up or dropping objects were implemented.
#define user
Traceback (most recent call last):
  File "/pfs/data6/home/tu/tu_tu/tu_zxoau23/pomdp_coder/uncertain_worms/policies/base_policy.py", line 194, in requery
    code_obj = compile(code_str, filename=filename, mode="exec")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "generated_code_transition_func000.py", line 12
    def rotate Direction(agent_dir, action):
               ^^^^^^^^^
SyntaxError: expected '('

