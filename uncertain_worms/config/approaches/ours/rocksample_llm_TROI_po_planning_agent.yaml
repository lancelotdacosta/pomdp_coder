defaults:
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

max_steps: 40
# max_steps: 20
num_episodes: 10
seed: 0
gamma: 0.98
save_log: false
replay_path: None

actions: &actions [0, 1, 2, 3, 4, 5, 6, 7]
env:
  _target_: uncertain_worms.environments.rocksample.rocksample_env.RockSampleEnv
  fully_obs: false

agent:
  _target_: uncertain_worms.policies.partially_obs_planning_agent.LLMPartiallyObsPlanningAgent
  max_attempts: 250000
  num_particles: 50
  # num_model_attempts: 25
  num_model_attempts: 5  # faster for testing, change back to 25 for final runs
  num_initial_model_samples: 1000
  fully_obs: false
  env_code_path: environments/rocksample
  actions: *actions
  learn_transition: true
  learn_initial: true
  learn_reward: true
  learn_observation: true
  use_online: true
  use_offline: true
  dataset_path: "environments/rocksample/trajectory_data/rocksample_data.pkl"
  env_description: ""

  empty_state:
    _target_: uncertain_worms.environments.rocksample.rocksample_env.rocksample_empty_state_gen

  empty_observation:
    _target_: uncertain_worms.environments.rocksample.rocksample_env.rocksample_empty_observation_gen

  planner:
    _target_: uncertain_worms.planners.PO_DAStar.PO_DAStar

    lambda_coeff: 0.1
    entropy_coeff: 0.0
    max_expansions: 20000
    num_rollouts: 1
    continue_search_on_goal: true
    
    actions: *actions

    empty_observation:
      _target_: uncertain_worms.environments.rocksample.rocksample_env.rocksample_empty_observation_gen
      
    initial_model: 
      _target_: uncertain_worms.structs.empty_initial_model_gen
    transition_model: 
      _target_: uncertain_worms.structs.empty_transition_model_gen
    reward_model:
      _target_: uncertain_worms.structs.empty_reward_model_gen
    observation_model:
      _target_: uncertain_worms.structs.empty_observation_model_gen
